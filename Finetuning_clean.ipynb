{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "\n",
    "from torch.utils.data.dataset import Dataset\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.autograd import Variable\n",
    "torch.backends.cudnn.bencmark = True\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import os,sys,cv2,random,datetime,time,math\n",
    "import argparse\n",
    "import numpy as np\n",
    "\n",
    "from net_s3fdnew import *\n",
    "from s3fd import *\n",
    "from bbox import *\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class CelebDataset(Dataset):\n",
    "    \"\"\"Dataset wrapping images and target labels\n",
    "    Arguments:\n",
    "        A CSV file path\n",
    "        Path to image folder\n",
    "        Extension of images\n",
    "        PIL transforms\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, csv_path, img_path, img_ext, transform=None):\n",
    "    \n",
    "        tmp_df = pd.read_csv(csv_path)\n",
    "        assert tmp_df['Image_Name'].apply(lambda x: os.path.isfile(img_path + x + img_ext)).all(), \\\n",
    "\"Some images referenced in the CSV file were not found\"\n",
    "        \n",
    "        self.mlb = MultiLabelBinarizer()\n",
    "        self.img_path = img_path\n",
    "        self.img_ext = img_ext\n",
    "        self.transform = transform\n",
    "\n",
    "        self.X_train = tmp_df['Image_Name']\n",
    "        self.y_train = self.mlb.fit_transform(tmp_df['Gender'].str.split()).astype(np.float32)\n",
    "    def __getitem__(self, index):\n",
    "        img = cv2.imread(self.img_path + self.X_train[index] + self.img_ext)\n",
    "        img = cv2.resize(img, (256,256))\n",
    "        img = img - np.array([104,117,123])\n",
    "        img = img.transpose(2, 0, 1)\n",
    "        \n",
    "        #img = img.reshape((1,)+img.shape)\n",
    "        img = torch.from_numpy(img).float()\n",
    "        #img = Variable(torch.from_numpy(img).float(),volatile=True)\n",
    "        \n",
    "        #if self.transform is not None:\n",
    "        #    img = self.transform(img)\n",
    "        \n",
    "        label = torch.from_numpy(self.y_train[index])\n",
    "        return img, label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X_train.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformations = transforms.Compose(\n",
    "    [\n",
    "     transforms.ToTensor()\n",
    "     \n",
    "     #transforms.Normalize(mean=[104,117,123])\n",
    "     ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = \"index.csv\"\n",
    "img_path = \"data/Celeb_Small_Dataset/\"\n",
    "img_ext = \".jpg\"\n",
    "dset = CelebDataset(train_data,img_path,img_ext,transformations)\n",
    "train_loader = DataLoader(dset,\n",
    "                          batch_size=1,\n",
    "                          shuffle=True,\n",
    "                          num_workers=1 # 1 for CUDA\n",
    "                         # pin_memory=True # CUDA only\n",
    "                         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save(model, optimizer, loss, filename, loss_array):\n",
    "    save_dict = {\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'loss': loss.data[0],\n",
    "        'loss_array': loss_array\n",
    "        }\n",
    "    \n",
    "    torch.save(save_dict, filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, criterion, optimizer, num_classes, num_epochs = 100):\n",
    "    loss_array={0:[], 1:[],2:[],3:[]}\n",
    "    for epoch in range(num_epochs):\n",
    "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
    "        print('-' * 10)\n",
    "        \n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "\n",
    "        for i,(img,label) in enumerate(train_loader):\n",
    "            img = img.view((1,)+img.shape[1:])\n",
    "            if use_cuda:\n",
    "                data, target = Variable(img.cuda()), Variable(torch.Tensor(label).cuda())\n",
    "            else:\n",
    "                data, target = Variable(img), Variable(torch.Tensor(label))\n",
    "            target = target.view(num_classes,1)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            # would need to add the code here to classify the \n",
    "            olist = model(data)\n",
    "            for p in range(len(olist)):\n",
    "                olist[p]= F.softmax(olist[p])\n",
    "                \n",
    "            for j in range(int(len(olist)/2)):\n",
    "                ocls,gen = olist[j*2].data.cpu(),olist[j*2+1].data.cpu()\n",
    "                FB, FC, FH, FW= ocls.size()\n",
    "                stride= 2**(j+2)\n",
    "                anchor= stride*4\n",
    "                for Findex in range(FH*FW):\n",
    "                    windex,hindex = Findex%FW,Findex//FW\n",
    "                    axc,ayc = stride/2+windex*stride,stride/2+hindex*stride\n",
    "                    score = ocls[0,1,hindex,windex]\n",
    "                    if score<0.05: continue\n",
    "\n",
    "                    loss = criterion(olist[j*2+1], target)\n",
    "\n",
    "                    loss.backward(retain_graph=True)\n",
    "                    optimizer.step()\n",
    "                    running_loss += loss.data[0]\n",
    "                    loss_array[j].append(loss.data[0])\n",
    "                    break\n",
    "                    \n",
    "            if i%50==0:\n",
    "                print(\"Reached iteration \",i)\n",
    "                running_loss += loss.data[0]\n",
    "\n",
    "            \n",
    "        if epoch % 10 == 0:\n",
    "            save(model, optimizer, loss, 'faceRecog.saved.model', loss_array)\n",
    "        print(running_loss)\n",
    "        with open('loss.csv', 'a') as file:      \n",
    "            file.writelines(str(running_loss)+ \" \")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "s3fd(\n",
       "  (conv1_1): Conv2d (3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (conv1_2): Conv2d (64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (conv2_1): Conv2d (64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (conv2_2): Conv2d (128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (conv3_1): Conv2d (128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (conv3_2): Conv2d (256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (conv3_3): Conv2d (256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (conv4_1): Conv2d (256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (conv4_2): Conv2d (512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (conv4_3): Conv2d (512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (conv5_1): Conv2d (512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (conv5_2): Conv2d (512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (conv5_3): Conv2d (512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (fc6): Conv2d (512, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(3, 3))\n",
       "  (fc7): Conv2d (1024, 1024, kernel_size=(1, 1), stride=(1, 1))\n",
       "  (conv6_1): Conv2d (1024, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "  (conv6_2): Conv2d (256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "  (conv7_1): Conv2d (512, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "  (conv7_2): Conv2d (128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "  (fc_1): Linear(in_features=2304, out_features=2)\n",
       "  (conv3_3_norm): L2Norm(\n",
       "  )\n",
       "  (conv4_3_norm): L2Norm(\n",
       "  )\n",
       "  (conv5_3_norm): L2Norm(\n",
       "  )\n",
       "  (conv3_3_norm_mbox_conf): Conv2d (256, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (conv3_3_norm_mbox_loc): Conv2d (256, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (conv4_3_norm_mbox_conf): Conv2d (512, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (conv4_3_norm_mbox_loc): Conv2d (512, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (conv4_3_norm_gender): Conv2d (512, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (l4_3): Linear(in_features=2048, out_features=50)\n",
       "  (l4_f): Linear(in_features=50, out_features=2)\n",
       "  (conv5_3_norm_mbox_conf): Conv2d (512, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (conv5_3_norm_mbox_loc): Conv2d (512, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (conv5_3_norm_gender): Conv2d (512, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (l5_3): Linear(in_features=2048, out_features=50)\n",
       "  (l5_f): Linear(in_features=50, out_features=2)\n",
       "  (fc7_mbox_conf): Conv2d (1024, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (fc7_mbox_loc): Conv2d (1024, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (convfc7_norm_gender): Conv2d (1024, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (lfc7): Linear(in_features=288, out_features=50)\n",
       "  (lfc7f): Linear(in_features=50, out_features=2)\n",
       "  (conv6_2_mbox_conf): Conv2d (512, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (conv6_2_mbox_loc): Conv2d (512, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (conv6_2_norm_gender): Conv2d (512, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (l6_2): Linear(in_features=288, out_features=50)\n",
       "  (l6_f): Linear(in_features=50, out_features=2)\n",
       "  (conv7_2_mbox_conf): Conv2d (256, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (conv7_2_mbox_loc): Conv2d (256, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "use_cuda = True\n",
    "myModel.eval()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "num_classes = 2\n",
    "myModel = s3fd(num_classes)\n",
    "loadedModel = torch.load('s3fd_convert.pth')\n",
    "newModel = myModel.state_dict()\n",
    "pretrained_dict = {k: v for k, v in loadedModel.items() if k in newModel}\n",
    "newModel.update(pretrained_dict)\n",
    "myModel.load_state_dict(newModel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.BCELoss()\n",
    "\n",
    "for param in myModel.parameters():\n",
    "    param.requires_grad = False\n",
    "    \n",
    "state_dict = myModel.state_dict()\n",
    "\n",
    "myModel.conv4_3_norm_gender = nn.Conv2d(512, 2, kernel_size=3, stride=1, padding=1)\n",
    "myModel.l4_3 = nn.Linear(2048,50)\n",
    "myModel.l4_f = nn.Linear(50,num_classes)\n",
    "l= loadedModel['conv4_3_norm_mbox_conf.weight']\n",
    "l[1]=l[0]\n",
    "state_dict['conv4_3_norm_gender.weight']=l\n",
    "\n",
    "myModel.conv5_3_norm_gender = nn.Conv2d(512, 2, kernel_size=3, stride=1, padding=1)\n",
    "myModel.l5_3 = nn.Linear(2048,50)\n",
    "myModel.l5_f = nn.Linear(50,num_classes)\n",
    "l= loadedModel['conv5_3_norm_mbox_conf.weight']\n",
    "l[1]=l[0]\n",
    "state_dict['conv5_3_norm_gender.weight']=l\n",
    "\n",
    "myModel.convfc7_norm_gender = nn.Conv2d(1024, 2, kernel_size=3, stride=1, padding=1)\n",
    "myModel.lfc7 = nn.Linear(288,50)\n",
    "myModel.lfc7f = nn.Linear(50,num_classes)\n",
    "l= loadedModel['fc7_mbox_conf.weight']\n",
    "l[1]=l[0]\n",
    "state_dict['convfc7_norm_gender.weight']=l\n",
    "\n",
    "myModel.conv6_2_norm_gender = nn.Conv2d(512, 2, kernel_size=3, stride=1, padding=1)\n",
    "myModel.l6_2 = nn.Linear(288,50)\n",
    "myModel.l6_f = nn.Linear(50,num_classes)\n",
    "l= loadedModel['conv6_2_mbox_conf.weight']\n",
    "l[1]=l[0]\n",
    "state_dict['conv6_2_norm_gender.weight']=l\n",
    "\n",
    "myModel.load_state_dict(state_dict)\n",
    "\n",
    "\n",
    "optimizer = optim.SGD(filter(lambda p: p.requires_grad,myModel.parameters()), lr=0.01, momentum=0.9)\n",
    "if use_cuda:\n",
    "    myModel = myModel.cuda()\n",
    "model_ft = train_model(myModel, criterion, optimizer, num_classes, num_epochs=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### retrain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/999\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/ipykernel/__main__.py:22: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/nn/functional.py:1168: UserWarning: Using a target size (torch.Size([2, 1])) that is different to the input size (torch.Size([1, 2])) is deprecated. Please ensure they have the same size.\n",
      "  \"Please ensure they have the same size.\".format(target.size(), input.size()))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reached iteration  0\n",
      "Reached iteration  50\n",
      "Reached iteration  100\n",
      "Reached iteration  150\n",
      "1486.5665263533592\n",
      "Epoch 1/999\n",
      "----------\n",
      "Reached iteration  0\n",
      "Reached iteration  50\n",
      "Reached iteration  100\n",
      "Reached iteration  150\n",
      "1486.4904508143663\n",
      "Epoch 2/999\n",
      "----------\n",
      "Reached iteration  0\n",
      "Reached iteration  50\n",
      "Reached iteration  100\n",
      "Reached iteration  150\n",
      "1486.2629038468003\n",
      "Epoch 3/999\n",
      "----------\n",
      "Reached iteration  0\n",
      "Reached iteration  50\n",
      "Reached iteration  100\n",
      "Reached iteration  150\n",
      "1484.5968556702137\n",
      "Epoch 4/999\n",
      "----------\n",
      "Reached iteration  0\n",
      "Reached iteration  50\n",
      "Reached iteration  100\n",
      "Reached iteration  150\n",
      "1489.8100982755423\n",
      "Epoch 5/999\n",
      "----------\n",
      "Reached iteration  0\n",
      "Reached iteration  50\n",
      "Reached iteration  100\n",
      "Reached iteration  150\n",
      "1489.3377259969711\n",
      "Epoch 6/999\n",
      "----------\n",
      "Reached iteration  0\n",
      "Reached iteration  50\n",
      "Reached iteration  100\n",
      "Reached iteration  150\n",
      "1488.5271205753088\n",
      "Epoch 7/999\n",
      "----------\n",
      "Reached iteration  0\n",
      "Reached iteration  50\n",
      "Reached iteration  100\n",
      "Reached iteration  150\n",
      "1490.2437595799565\n",
      "Epoch 8/999\n",
      "----------\n",
      "Reached iteration  0\n",
      "Reached iteration  50\n",
      "Reached iteration  100\n",
      "Reached iteration  150\n",
      "1487.2756537273526\n",
      "Epoch 9/999\n",
      "----------\n",
      "Reached iteration  0\n",
      "Reached iteration  50\n",
      "Reached iteration  100\n",
      "Reached iteration  150\n",
      "1490.1684443950653\n",
      "Epoch 10/999\n",
      "----------\n",
      "Reached iteration  0\n",
      "Reached iteration  50\n",
      "Reached iteration  100\n",
      "Reached iteration  150\n",
      "1488.2425879389048\n",
      "Epoch 11/999\n",
      "----------\n",
      "Reached iteration  0\n",
      "Reached iteration  50\n",
      "Reached iteration  100\n",
      "Reached iteration  150\n",
      "1491.0108285397291\n",
      "Epoch 12/999\n",
      "----------\n",
      "Reached iteration  0\n",
      "Reached iteration  50\n",
      "Reached iteration  100\n",
      "Reached iteration  150\n",
      "1485.366347193718\n",
      "Epoch 13/999\n",
      "----------\n",
      "Reached iteration  0\n",
      "Reached iteration  50\n",
      "Reached iteration  100\n",
      "Reached iteration  150\n",
      "1488.29884480685\n",
      "Epoch 14/999\n",
      "----------\n",
      "Reached iteration  0\n",
      "Reached iteration  50\n",
      "Reached iteration  100\n",
      "Reached iteration  150\n",
      "1485.451784208417\n",
      "Epoch 15/999\n",
      "----------\n",
      "Reached iteration  0\n",
      "Reached iteration  50\n",
      "Reached iteration  100\n",
      "Reached iteration  150\n",
      "1488.8801397681236\n",
      "Epoch 16/999\n",
      "----------\n",
      "Reached iteration  0\n",
      "Reached iteration  50\n",
      "Reached iteration  100\n",
      "Reached iteration  150\n",
      "1486.429640159011\n",
      "Epoch 17/999\n",
      "----------\n",
      "Reached iteration  0\n",
      "Reached iteration  50\n",
      "Reached iteration  100\n",
      "Reached iteration  150\n",
      "1490.320639938116\n",
      "Epoch 18/999\n",
      "----------\n",
      "Reached iteration  0\n",
      "Reached iteration  50\n",
      "Reached iteration  100\n",
      "Reached iteration  150\n",
      "1486.5772733688354\n",
      "Epoch 19/999\n",
      "----------\n",
      "Reached iteration  0\n",
      "Reached iteration  50\n",
      "Reached iteration  100\n",
      "Reached iteration  150\n",
      "1485.1349801793694\n",
      "Epoch 20/999\n",
      "----------\n",
      "Reached iteration  0\n",
      "Reached iteration  50\n",
      "Reached iteration  100\n",
      "Reached iteration  150\n",
      "1487.111779987812\n",
      "Epoch 21/999\n",
      "----------\n",
      "Reached iteration  0\n",
      "Reached iteration  50\n",
      "Reached iteration  100\n",
      "Reached iteration  150\n",
      "1488.0408929139376\n",
      "Epoch 22/999\n",
      "----------\n"
     ]
    }
   ],
   "source": [
    "criterion = nn.BCELoss()\n",
    "\n",
    "num_classes = 2\n",
    "myModel = s3fd(num_classes)\n",
    "file = torch.load('faceRecog.saved.model')\n",
    "pretrained_dict = file['model_state_dict']\n",
    "myModel.load_state_dict(pretrained_dict)\n",
    "\n",
    "optimizer = optim.SGD(filter(lambda p: p.requires_grad,myModel.parameters()), lr=0.01, momentum=0.9)\n",
    "if use_cuda:\n",
    "    myModel = myModel.cuda()\n",
    "model_ft = train_model(myModel, criterion, optimizer, num_classes, num_epochs=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform(img_path):\n",
    "        img = cv2.imread(img_path)\n",
    "        img = cv2.resize(img, (256,256))\n",
    "        img = img - np.array([104,117,123])\n",
    "        img = img.transpose(2, 0, 1)\n",
    "        \n",
    "        img = img.reshape((1,)+img.shape)\n",
    "        img = torch.from_numpy(img).float()\n",
    "        \n",
    "        return Variable(img.cuda())\n",
    "myModel = myModel.cuda()\n",
    "testImage1 = transform('data/Test/TestCeleb_3/24-FaceId-0.jpg')\n",
    "testImage2 = transform('data/Test/TestCeleb_4/26-FaceId-0.jpg')\n",
    "testImage3 = transform('data/Test/TestCeleb_4/27-FaceId-0.jpg')\n",
    "testImage4 = transform('data/Test/TestCeleb_10/25-FaceId-0.jpg')\n",
    "testImage5 = transform('data/Test/TestCeleb_10/26-FaceId-0.jpg')\n",
    "testImage6 = transform('data/Test/TestCeleb_10/24-FaceId-0.jpg')\n",
    "\n",
    "__,ge2,__,ge3,__,ge4,__,ge5 = myModel(testImage1)\n",
    "olist = myModel(testImage6)\n",
    "for p in range(len(olist)):\n",
    "    olist[p]= F.softmax(olist[p])\n",
    "                \n",
    "for j in range(int(len(olist)/2)):\n",
    "    ocls,gen = olist[j*2].data.cpu(),olist[j*2+1].data.cpu()\n",
    "    FB, FC, FH, FW= ocls.size()\n",
    "    stride= 2**(j+2)\n",
    "    anchor= stride*4\n",
    "    for Findex in range(FH*FW):\n",
    "        windex,hindex = Findex%FW,Findex//FW\n",
    "        axc,ayc = stride/2+windex*stride,stride/2+hindex*stride\n",
    "        score = ocls[0,1,hindex,windex]\n",
    "        if score<0.05: continue\n",
    "        print(gen)\n",
    "        break\n",
    "\n",
    "# output2 = myModel(testImage2)\n",
    "# output3 = myModel(testImage2)\n",
    "# output4 = myModel(testImage4)\n",
    "# output5 = myModel(testImage5)\n",
    "# output6 = myModel(testImage6)\n",
    "# print(\"testImage1 - \",output1)\n",
    "# print(\"testImage2 - \",output2)\n",
    "# print(\"testImage3 - \",output3)\n",
    "# print(\"testImage1 - \",output4)\n",
    "# print(\"testImage2 - \",output5)\n",
    "# print(\"testImage3 - \",output6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ignore below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "testImage1 = transform('data/Test/TestCeleb_5/24-FaceId-0.jpg')\n",
    "output1 = myModel(testImage1)\n",
    "print(\"testImage1 - \",output1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = torch.load('faceRecog.saved.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('loss.csv', 'r') as file:\n",
    "    x= file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "odict_keys(['conv1_1.weight', 'conv1_1.bias', 'conv1_2.weight', 'conv1_2.bias', 'conv2_1.weight', 'conv2_1.bias', 'conv2_2.weight', 'conv2_2.bias', 'conv3_1.weight', 'conv3_1.bias', 'conv3_2.weight', 'conv3_2.bias', 'conv3_3.weight', 'conv3_3.bias', 'conv4_1.weight', 'conv4_1.bias', 'conv4_2.weight', 'conv4_2.bias', 'conv4_3.weight', 'conv4_3.bias', 'conv5_1.weight', 'conv5_1.bias', 'conv5_2.weight', 'conv5_2.bias', 'conv5_3.weight', 'conv5_3.bias', 'fc6.weight', 'fc6.bias', 'fc7.weight', 'fc7.bias', 'conv6_1.weight', 'conv6_1.bias', 'conv6_2.weight', 'conv6_2.bias', 'conv7_1.weight', 'conv7_1.bias', 'conv7_2.weight', 'conv7_2.bias', 'fc_1.weight', 'fc_1.bias', 'conv3_3_norm.weight', 'conv4_3_norm.weight', 'conv5_3_norm.weight', 'conv3_3_norm_mbox_conf.weight', 'conv3_3_norm_mbox_conf.bias', 'conv3_3_norm_mbox_loc.weight', 'conv3_3_norm_mbox_loc.bias', 'conv4_3_norm_mbox_conf.weight', 'conv4_3_norm_mbox_conf.bias', 'conv4_3_norm_mbox_loc.weight', 'conv4_3_norm_mbox_loc.bias', 'conv4_3_norm_gender.weight', 'conv4_3_norm_gender.bias', 'l4_3.weight', 'l4_3.bias', 'l4_f.weight', 'l4_f.bias', 'conv5_3_norm_mbox_conf.weight', 'conv5_3_norm_mbox_conf.bias', 'conv5_3_norm_mbox_loc.weight', 'conv5_3_norm_mbox_loc.bias', 'conv5_3_norm_gender.weight', 'conv5_3_norm_gender.bias', 'l5_3.weight', 'l5_3.bias', 'l5_f.weight', 'l5_f.bias', 'fc7_mbox_conf.weight', 'fc7_mbox_conf.bias', 'fc7_mbox_loc.weight', 'fc7_mbox_loc.bias', 'convfc7_norm_gender.weight', 'convfc7_norm_gender.bias', 'lfc7.weight', 'lfc7.bias', 'lfc7f.weight', 'lfc7f.bias', 'conv6_2_mbox_conf.weight', 'conv6_2_mbox_conf.bias', 'conv6_2_mbox_loc.weight', 'conv6_2_mbox_loc.bias', 'conv6_2_norm_gender.weight', 'conv6_2_norm_gender.bias', 'l6_2.weight', 'l6_2.bias', 'l6_f.weight', 'l6_f.bias', 'conv7_2_mbox_conf.weight', 'conv7_2_mbox_conf.bias', 'conv7_2_mbox_loc.weight', 'conv7_2_mbox_loc.bias'])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file['model_state_dict'].keys()\n",
    "\n",
    "loadedModel = torch.load('s3fd_convert.pth')\n",
    "l=torch.load('s3fd_convert.pth')['conv4_3_norm_mbox_conf.weight']\n",
    "l[1]=l[0]\n",
    "l.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
